#!/usr/bin/env python3

import logging
import argparse
import sys
import yaml
import os.path
import os
import subprocess
import fcntl
import json
import time

import ctypes
import struct
import datetime

class BlockHeader(ctypes.BigEndianStructure):
    _pack_ = 1
    _fields_ = [
        ("CheckSum", ctypes.c_uint32),
        ("BlockSize", ctypes.c_uint32),
        ('BlockNumber', ctypes.c_uint32),
        ('ID', ctypes.c_char * 4),
        ('VolSessionId', ctypes.c_uint32),
        ('VolSessionTime', ctypes.c_uint32),
    ]

class RecordHeader(ctypes.BigEndianStructure):
    _pack_ = 1
    _fields_ = [
        ("FileIndex", ctypes.c_int32),
        ("Stream", ctypes.c_int32),
        ('DataSize', ctypes.c_uint32),
    ]

class BaresVolumeMetadataReader(object):

    @staticmethod    
    def _readString(f, max_chars=512):
        chars = b""
        for n in range(0, max_chars - 1):
            c = BaresVolumeMetadataReader._readData(f, 'char')
            if ord(c) == 0:
                return "".join(str(chars, 'utf-8'))
            chars += c
        return chars

    @staticmethod
    def _readData(f, typeName):
        typeNames = {
            'int8'   :'>b', 'uint8'  :'>B',
            'int16'  :'>h', 'uint16' :'>H',
            'int32'  :'>i', 'uint32' :'>I',
            'int64'  :'>q', 'uint64' :'>Q',
            'float'  :'>f', 'float16' :'>e',
            'double' :'>d',
            'char'   :'>c',
        }

        typeFormat = typeNames[typeName.lower()]
        typeSize = struct.calcsize(typeFormat)
        bValue = f.read(typeSize)
        return struct.unpack(typeFormat, bValue)[0]

    @staticmethod
    def _readDateTime(f):
        return datetime.datetime.fromtimestamp(float(BaresVolumeMetadataReader._readData(f, 'double'))).strftime('%Y-%m-%d %H:%M:%S.%f')

    @staticmethod
    def _ctypes_structure_to_dict(name, structure):
        ret = dict()
        for field_name, field_type in structure._fields_:
            val = getattr(structure, field_name)
            if isinstance(val, bytes):
                val = str(val, 'utf-8')
            ret[field_name] = str(val)
        return ret

    @staticmethod
    def _debug_ctypes_structure(name, structure):
        logging.debug("")
        logging.debug("== %s ==" % name)
        for field_name, field_type in structure._fields_:
            logging.debug(field_name + "=" + str(getattr(structure, field_name)))

    @staticmethod
    def _debug_dict(name, h):
        logging.debug("")
        logging.debug("== %s ==" % name)
        for field_name, value in h.items():
            logging.debug(field_name + "=" + str(value))

    @staticmethod
    def _read_ctype_structure(f, name, struct_class):
        structure = struct_class.from_buffer_copy(f.read(ctypes.sizeof(struct_class)))
        struct_dict = BaresVolumeMetadataReader._ctypes_structure_to_dict(name, structure)
        BaresVolumeMetadataReader._debug_dict(name, struct_dict)
        return struct_dict
    
    @staticmethod
    def _read_volume_label(f, name):
        volume_header = dict()
        volume_header['Id'] = BaresVolumeMetadataReader._readString(f, 32)
        volume_header['VerNum'] = BaresVolumeMetadataReader._readData(f, 'uint32')
        volume_header['label_btime'] = BaresVolumeMetadataReader._readDateTime(f)
        volume_header['write_btime'] = BaresVolumeMetadataReader._readDateTime(f)
        volume_header['write_date'] = BaresVolumeMetadataReader._readDateTime(f)
        volume_header['write_time'] = BaresVolumeMetadataReader._readDateTime(f)
        volume_header['VolName'] = BaresVolumeMetadataReader._readString(f)
        BaresVolumeMetadataReader._debug_dict("VolumeLabel", volume_header)

        return volume_header

    @staticmethod
    def readHeader(vol_file):
        with open(vol_file, "rb") as f:
            ret = dict()
            ret['BlockHeader'] = BaresVolumeMetadataReader._read_ctype_structure(f, "BlockHeader", BlockHeader)
            ret['RecordHeader'] = BaresVolumeMetadataReader._read_ctype_structure(f, "RecordHeader", RecordHeader)
            ret['VolumeLabel'] = BaresVolumeMetadataReader._read_volume_label(f, "VolumeLabel")

            return ret

    @staticmethod
    def getVolumeLabel(vol_file):
        return BaresVolumeMetadataReader.readHeader(vol_file)['VolumeLabel']['VolName']

class Rclone(object):
    """
    Rclone contains logic to convert simple commands to full rclone command line
    calls with error handling
    """
    #=====================================================
    # Default params config
    #=====================================================
    rclone     = '/usr/bin/rclone'
    config     = '/etc/bareos/rclone.conf'
    logFile    = '/var/log/bareos/rclone.log'
    logChanger = '/var/log/bareos/rclone-changer.log'
    lockFile   = '/data/backup/bareos/.rclonelock'
    stateFile  = '/data/backup/bareos/rclone-changer.state'
    slots      = 200
    #=====================================================

    options = []

    @staticmethod
    def _exec_raw_(*args) -> subprocess.CompletedProcess:
        """
        Generic call to execute rclone for use internally by the public rclone command subroutines

        :param args: simple command list to pass to rclone after calculated options
        :return: True on success.  
        """

        # Compile all the option flags (some implied, some passed by the user) into a single list
        # -c / --checksum has a weird behaviour (tested with sftp backend): If source and destination have no matching blocks,
        # rclone falls back to comparing the size. As pre-labeled volumes have the same size, this leads to volumes being
        # never copied. Therefore we use ignore-times and size instead. Under some circumstances, more data is copied over
        # the wire. However, this behaviour is better than introducing weird bugs.
        flags = Rclone.options + [
            '--config=%s' % Rclone.config, 
            '--log-file=%s' % Rclone.logFile, 
            '-q', 
            '--ignore-times', 
            '--ignore-size',
            '--retries', '2880',
            '--retries-sleep', '30s'
        ]
        logging.debug('Executing: %s %s %s' % (Rclone.rclone, ' '.join(flags), ' '.join(args) ))

        # Actually call rclone with options, plus the operation specific arguments
        callList = [ Rclone.rclone ] + flags + __builtins__.list(args)
        logging.debug('Executing: callList=%s' % callList)
        out = subprocess.run(args=callList, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if out.returncode != 0:
            logging.debug('Result: returncode=%s' % out.returncode)
            logging.error(out.stderr)
            raise Exception('Rclone command failed.  Check log %s' % Rclone.logFile )
        
        return out

    @staticmethod
    def _exec_(*args):
        """
        Generic call to execute rclone for use internally by the public rclone command subroutines

        :param args: simple command list to pass to rclone after calculated options
        :return: True on success.  
        """

        r = Rclone._exec_raw_(*args)
        # Basic error handling
        if r.returncode != 0:
            raise Exception('Rclone command failed.  Check log %s' % Rclone.logFile )
        else:
            return True

    @staticmethod
    def copy(source,destination):
        """
        Wrap a call to __exec__ to copy a file


        :param source: The source path/file
        :param destination: The destination path
        :returns: True on success
        """
        return Rclone._exec_('copy', source, destination)

    @staticmethod
    def move(source,destination):
        """
        Wrap a call to __exec__ to move a file


        :param source: The source path/file
        :param destination: The destination path
        :returns: True on success
        """
        return Rclone._exec_('move', source, destination)

    @staticmethod
    def exists(path):
        """
        Wrap a call to __exec__ to test if a file exists


        :param path: The path to test
        :returns: True if the file exists, False if it does not
        """
        # TODO: This logic sucks for the initial run when no files exist yet, 
        # because it fails and times out.
        # Maybe it can be dropped, because retry logic is implemented with rclone
        # parameters
        for retry in range(0, 30):
            try:
                r = Rclone._exec_raw_('lsjson', path)
                logging.debug('lsjson returned: %s' % r.stdout)
                files = json.loads(r.stdout)
                return len(files) > 0
            except Exception as e:
                logging.error("lsjson for %s failed: %s. Retry %s" % (path, e, retry))
                time.sleep(1)
        return False

class State(object):
    """
    State is a simple container class for the state of the tape changer, which includes 
    slot count and which slot is loaded
    """
    loadedSlot = 0
    slots = Rclone.slots


class CommandOpts(object):
    """
    CommandOpts is a simple class containing the bacula-centric command to pass to subsequent
    subroutines.
    """
    changer = None
    command = None
    slot    = None
    archive = None

    def __init__(self, params):
        self.changer = params.changerdevice
        self.slot    = params.slot
        self.archive = params.archivedevice

def parseArgs(args):

    parser = argparse.ArgumentParser(formatter_class = argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('--lockfile', default = Rclone.lockFile,
        help='Lock file to prevent more than one rclone call from running and mucking up the state file')

    parser.add_argument('--logfile', default = Rclone.logChanger, 
        help='Where to store the rclone-changer log file (not the rclone log itself')

    parser.add_argument('--rclone_path', default = Rclone.rclone,
        help='Path to rclone binary')

    parser.add_argument('--rclone_options', default = False,
        help='All additional rclone options you need to pass in a single argument')

    parser.add_argument('--rclone_config', default = Rclone.config,
        help='Path to rclone config for your cloud storage of choice (result of `rclone config`)')

    parser.add_argument('--rclone_log', default = Rclone.logFile,
        help='Where to have rclone itself log to')

    parser.add_argument('--statefile', default = Rclone.stateFile,
        help='Where to store our "changer" state')

    parser.add_argument('changerdevice')
    parser.add_argument('command')
    parser.add_argument('slot')
    parser.add_argument('archivedevice')

    return parser.parse_args()

def loaded(opts, state):
    print(state.loadedSlot)

def load(opts, state):
    logging.info("Loading volume %s into %s from %s" % (opts.slot, opts.archive, opts.changer))
    
    source = "/".join([opts.changer, opts.slot, os.path.basename(opts.archive)])
    destination = os.path.dirname(opts.archive)

    # Since I'm unsure if a standard SCSI changer would allow you to load a tape into an 
    # occupied device, support that behavior just in case.  Basically if the 'drive' is not 
    # empty and we are told to load a tape into it, unload it implicitly
    if state.loadedSlot != 0 and state.loadedSlot != opts.slot:
        logging.warning("Asked to load volume %s into %s but %s is loaded.  Unloading %s" % 
            (opts.slot, opts.archive, state.loadedSlot, state.loadedSlot))

        unloadOpts = argparse.Namespace()
        unloadOpts.slot = state.loadedSlot
        unloadOpts.archive = opts.archive
        unloadOpts.changer = opts.changer
        unload(unloadOpts, state)
    
    # We've been told to load a slot that is already loaded.  Do nothing.
    if state.loadedSlot == opts.slot:    
        logging.info("Volume %s already loaded at %s from %s" % (opts.slot, opts.archive, opts.changer))
        return

    logging.info("Copying %s to %s for load" % (source, destination))

    # Check if the source file exists.  Copy it locally if it does.
    if Rclone.exists(source):
        Rclone.copy(source, destination)
        vol_label = BaresVolumeMetadataReader.getVolumeLabel(opts.archive)
        formatted = format_label(opts.slot)
        if formatted != vol_label:
            msg = "Loaded: source=%s,slot=%s,volume_label=%s,formatted=%s but slot and volume label don't match." % (source, opts.slot, vol_label, formatted)
            logging.error(msg)
            raise Exception(msg)          
    # If the source doesn't exist we create it locally, which is fine.
    else:
        logging.warning("Volume %s didn't exist in %s.  Creating a new file" % (opts.slot, opts.changer))
        if os.path.exists(opts.archive):
            os.unlink(opts.archive)
        open(opts.archive,'w').close()
    
    # Update state after rclone command completion
    state.loadedSlot = opts.slot  
    logging.info("Loaded volume %s into %s from %s" % (opts.slot, opts.archive, opts.changer))

# Unload tape from a given slot
def unload(opts, state):
    logging.info("Unloading volume %s to %s from %s" % (opts.slot, opts.changer, opts.archive))

    source = opts.archive
    destination = "/".join([opts.changer, opts.slot])
  
    # If the specified volume is loaded, copy it to cloud storage and update state
    if opts.slot == state.loadedSlot:
        try:
            vol_label = BaresVolumeMetadataReader.getVolumeLabel(opts.archive)
            if format_label(opts.slot) != vol_label:
                msg = "Unload: source=%s,slot=%s,volume_label=%s, but slot and volume label don't match." % (source, opts.slot, vol_label)
                logging.error(msg)
                raise Exception(msg)
            Rclone.copy(source, destination)
            state.loadedSlot = 0
            logging.info("Unloaded volume %s to %s from %s" % (opts.slot, opts.changer, opts.archive))
        except Exception as e:
            msg = "Failed to unload volume %s: %s" % (opts.slot, e)
            logging.error(msg, e)
            raise Exception(msg)
    # If we're asked to unload a volume that isn't loaded (according to state file) something
    # has gone wrong
    else: 
        logging.error("Asked to unload volume %s but loaded volume is %s" % (opts.slot, state.loadedSlot))
        raise Exception("Volume %s not loaded" % opts.slot)

def format_label(i):
    return "VTAPE-{0:04n}".format(int(i))

# List changer slots with implied barcode data
def list(opts, state):
    logging.info("Listing volumes in %s" % opts.changer)
    for i in range(1, state.slots + 1):
        print("{0}:".format(i) + format_label(i)) 

# Return number of changer slots
def slots(opts, state):
    print(state.slots)

# Call required sub based on changer action
def dispatch(command, commandOpts, state):
    dispatch = {
     'loaded': loaded,
     'load'  : load,
     'unload': unload,
     'list'  : list,
     'slots' : slots,
    }
    dispatch[command](commandOpts, state)


if __name__ == "__main__":
    config = parseArgs(sys.argv)

    # Initialize logging
    logging.basicConfig(filename = config.logfile, level = logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.debug("Starting with args: %s" % vars(config))
    logging.info("Loading state from %s" % config.statefile)

    # Get a lock for this run to prevent state and log file corruption
    logging.debug("Locking at '%s'" % config.lockfile)
    lockF = open(config.lockfile, 'w')
    fcntl.lockf(lockF, fcntl.LOCK_EX)

    # Load state or start with no state assumed
    try:
        stateF = open(config.statefile)
        fcntl.flock(stateF, fcntl.LOCK_EX)
        state = yaml.load(stateF)
        stateF.close()
    except IOError as e:
        logging.warning("State file not present.  Starting from scratch and assuming no loaded volumes: %s" % e)
        state = State()

    # Set up rclone class for later
    Rclone.config = config.rclone_config
    Rclone.rclone = config.rclone_path
    if config.rclone_options:
        Rclone.options = str(config.rclone_options).split(' ')
    Rclone.logFile = config.rclone_log
    Rclone.localFile = config.archivedevice
    
    # Dispatch the command
    commandOpts = CommandOpts(config)
    dispatch(config.command, commandOpts, state)

    # Store final state and exit
    stateF = open(config.statefile,'w')
    fcntl.lockf(stateF, fcntl.LOCK_EX)
    yaml.dump(state, stateF, default_flow_style=False)
    stateF.close()
